# -*- coding: utf-8 -*-
"""aml_lstm_khushi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q6Y-81--PCMI4udKNhlXweJDs52MZeqg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive

drive.mount('/content/gdrive')

import warnings

def fxn():
    warnings.warn("deprecated", DeprecationWarning)

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    fxn()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

#Import data to dataframes
train_prot = pd.read_csv("/content/train_proteins (1).csv")
train_pep = pd.read_csv("/content/train_peptides (1).csv")

#Target
target = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']

#Categorical Features
cat_features = ['upd23b_clinical_state_on_medication']

#Numerical Features - Peptides
all_features_pep = train_pep.columns.tolist()
num_features_pep = all_features_pep
num_features_pep.remove('visit_id')

remove_list = target + cat_features
for item in remove_list:
    num_features_pep.remove(item)

#Peptide dataset
X_pep = train_pep[cat_features + num_features_pep].copy()
y_pep = train_pep[target].copy()

LSTM_target_pep = target.copy()
LSTM_cat_features_pep = cat_features.copy()
LSTM_num_features_pep = num_features_pep.copy()

LSTM_X_pep = X_pep.copy()
LSTM_y_pep = y_pep.copy()

#Clear datasets
LSTM_X_pep_train = None
LSTM_X_pep_test = None

#Split data into training (80%) and testing (20%)
LSTM_X_pep_train, LSTM_X_pep_test, LSTM_y_pep_train, LSTM_y_pep_test = train_test_split(LSTM_X_pep, LSTM_y_pep,
                                                                                test_size=0.2, random_state=42)

preprocess = make_column_transformer( (MinMaxScaler(), LSTM_num_features_pep),
                                      (OneHotEncoder(handle_unknown="ignore"), LSTM_cat_features_pep)
                                    )

LSTM_X_pep_train_preprocessed = preprocess.fit_transform(LSTM_X_pep_train)
LSTM_X_pep_test_preprocessed = preprocess.transform(LSTM_X_pep_test)

print(LSTM_X_pep_train.shape)
print(LSTM_X_pep_test.shape)

model = {}

from tensorflow.keras.layers import Masking

# ... (the rest of the code stays the same)

for t in LSTM_target_pep:
    print('Target:', t)
    X = LSTM_X_pep
    X_train = LSTM_X_pep_train_preprocessed
    y_train = LSTM_y_pep_train[t]
    X_test = LSTM_X_pep_test_preprocessed
    y_test = LSTM_y_pep_test[t]

    # Determine the maximum sequence length
    max_seq_length = max([len(seq) for seq in X_train])

    # Pad the sequences in the training set
    #X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype='float64')

    # Pad the sequences in the test set
    #X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype='float64')
    #print(X_train_padded.shape)
    #print(X_test_padded.shape)

     # Reshape input data into 3D array
    #X_train_padded = X_train_padded.reshape((X_train_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
    #X_test_padded = X_test_padded.reshape((X_test_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
     # Initialize the padded arrays
    X_train_padded = np.zeros((X_train.shape[0], max_seq_length, X_train.shape[1]))
    X_test_padded = np.zeros((X_test.shape[0], max_seq_length, X_test.shape[1]))

    # Manually pad the sequences in the training set
    for i, seq in enumerate(X_train):
        X_train_padded[i, :len(seq), :] = seq

    # Manually pad the sequences in the test set
    for i, seq in enumerate(X_test):
        X_test_padded[i, :len(seq), :] = seq

    # Create LSTM model
    model[t] = Sequential()
    print((max_seq_length, X_train_padded.shape[2]))
    print(max_seq_length)
    model[t].add(Masking(mask_value=0., input_shape=(max_seq_length, X_train_padded.shape[2])))
    model[t].add(LSTM(50, return_sequences=True))
    model[t].add(Dropout(0.2))
    model[t].add(LSTM(50, return_sequences=False))
    model[t].add(Dropout(0.3))
    model[t].add(Dense(1 ,activation='linear'))

    # Compile the model
    model[t].compile(optimizer='adam', loss='mse')

    # Train the model
    history=model[t].fit(X_train_padded, y_train, epochs=70, batch_size=64, validation_data=(X_test_padded, y_test))

    # Make predictions on the test set
    y_pred = model[t].predict(X_test_padded)


    r2=r2_score(y_test, y_pred)
    mse=mean_squared_error(y_test, y_pred)
    # Evaluate the model
    print('R2:',r2 )
    print('MSE:',mse )

    #plot loss graphs
    plt.figure()
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

from tensorflow.keras.layers import Masking
t='updrs_2'
print('Target:', t)
X = LSTM_X_pep
X_train = LSTM_X_pep_train_preprocessed
y_train = LSTM_y_pep_train[t]
X_test = LSTM_X_pep_test_preprocessed
y_test = LSTM_y_pep_test[t]

# Determine the maximum sequence length
max_seq_length = max([len(seq) for seq in X_train])

# Pad the sequences in the training set
#X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype='float64')

# Pad the sequences in the test set
#X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype='float64')
#print(X_train_padded.shape)
#print(X_test_padded.shape)

  # Reshape input data into 3D array
#X_train_padded = X_train_padded.reshape((X_train_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
#X_test_padded = X_test_padded.reshape((X_test_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
  # Initialize the padded arrays
X_train_padded = np.zeros((X_train.shape[0], max_seq_length, X_train.shape[1]))
X_test_padded = np.zeros((X_test.shape[0], max_seq_length, X_test.shape[1]))

# Manually pad the sequences in the training set
for i, seq in enumerate(X_train):
    X_train_padded[i, :len(seq), :] = seq

# Manually pad the sequences in the test set
for i, seq in enumerate(X_test):
    X_test_padded[i, :len(seq), :] = seq

# Create LSTM model
model[t] = Sequential()
print((max_seq_length, X_train_padded.shape[2]))
print(max_seq_length)
model[t].add(Masking(mask_value=0., input_shape=(max_seq_length, X_train_padded.shape[2])))
model[t].add(LSTM(50, return_sequences=True))
model[t].add(Dropout(0.2))
model[t].add(LSTM(50, return_sequences=False))
model[t].add(Dropout(0.3))
model[t].add(Dense(1 ,activation='linear'))

# Compile the model
model[t].compile(optimizer='adam', loss='mse')

# Train the model
history=model[t].fit(X_train_padded, y_train, epochs=70, batch_size=64, validation_data=(X_test_padded, y_test))

# Make predictions on the test set
y_pred = model[t].predict(X_test_padded)


r2=r2_score(y_test, y_pred)
mse=mean_squared_error(y_test, y_pred)
# Evaluate the model
print('R2:',r2 )
print('MSE:',mse )

#plot loss graphs
plt.figure()
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

from tensorflow.keras.layers import Masking
t='updrs_3'
print('Target:', t)
X = LSTM_X_pep
X_train = LSTM_X_pep_train_preprocessed
y_train = LSTM_y_pep_train[t]
X_test = LSTM_X_pep_test_preprocessed
y_test = LSTM_y_pep_test[t]

# Determine the maximum sequence length
max_seq_length = max([len(seq) for seq in X_train])

# Pad the sequences in the training set
#X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype='float64')

# Pad the sequences in the test set
#X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype='float64')
#print(X_train_padded.shape)
#print(X_test_padded.shape)

  # Reshape input data into 3D array
#X_train_padded = X_train_padded.reshape((X_train_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
#X_test_padded = X_test_padded.reshape((X_test_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
  # Initialize the padded arrays
X_train_padded = np.zeros((X_train.shape[0], max_seq_length, X_train.shape[1]))
X_test_padded = np.zeros((X_test.shape[0], max_seq_length, X_test.shape[1]))

# Manually pad the sequences in the training set
for i, seq in enumerate(X_train):
    X_train_padded[i, :len(seq), :] = seq

# Manually pad the sequences in the test set
for i, seq in enumerate(X_test):
    X_test_padded[i, :len(seq), :] = seq

# Create LSTM model
model[t] = Sequential()
print((max_seq_length, X_train_padded.shape[2]))
print(max_seq_length)
model[t].add(Masking(mask_value=0., input_shape=(max_seq_length, X_train_padded.shape[2])))
model[t].add(LSTM(50, return_sequences=True))
model[t].add(Dropout(0.2))
model[t].add(LSTM(50, return_sequences=False))
model[t].add(Dropout(0.3))
model[t].add(Dense(1 ,activation='linear'))

# Compile the model
model[t].compile(optimizer='adam', loss='mse')

# Train the model
history=model[t].fit(X_train_padded, y_train, epochs=70, batch_size=64, validation_data=(X_test_padded, y_test))

# Make predictions on the test set
y_pred = model[t].predict(X_test_padded)


r2=r2_score(y_test, y_pred)
mse=mean_squared_error(y_test, y_pred)
# Evaluate the model
print('R2:',r2 )
print('MSE:',mse )

#plot loss graphs
plt.figure()
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

from tensorflow.keras.layers import Masking
t='updrs_4'
print('Target:', t)
X = LSTM_X_pep
X_train = LSTM_X_pep_train_preprocessed
y_train = LSTM_y_pep_train[t]
X_test = LSTM_X_pep_test_preprocessed
y_test = LSTM_y_pep_test[t]

# Determine the maximum sequence length
max_seq_length = max([len(seq) for seq in X_train])

# Pad the sequences in the training set
#X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype='float64')

# Pad the sequences in the test set
#X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype='float64')
#print(X_train_padded.shape)
#print(X_test_padded.shape)

  # Reshape input data into 3D array
#X_train_padded = X_train_padded.reshape((X_train_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
#X_test_padded = X_test_padded.reshape((X_test_padded.shape[0], max_seq_length, X_train_padded.shape[1]))
  # Initialize the padded arrays
X_train_padded = np.zeros((X_train.shape[0], max_seq_length, X_train.shape[1]))
X_test_padded = np.zeros((X_test.shape[0], max_seq_length, X_test.shape[1]))

# Manually pad the sequences in the training set
for i, seq in enumerate(X_train):
    X_train_padded[i, :len(seq), :] = seq

# Manually pad the sequences in the test set
for i, seq in enumerate(X_test):
    X_test_padded[i, :len(seq), :] = seq

# Create LSTM model
model[t] = Sequential()
print((max_seq_length, X_train_padded.shape[2]))
print(max_seq_length)
model[t].add(Masking(mask_value=0., input_shape=(max_seq_length, X_train_padded.shape[2])))
model[t].add(LSTM(50, return_sequences=True))
model[t].add(Dropout(0.2))
model[t].add(LSTM(50, return_sequences=False))
model[t].add(Dropout(0.3))
model[t].add(Dense(1 ,activation='linear'))

# Compile the model
model[t].compile(optimizer='adam', loss='mse')

# Train the model
history=model[t].fit(X_train_padded, y_train, epochs=70, batch_size=64, validation_data=(X_test_padded, y_test))

# Make predictions on the test set
y_pred = model[t].predict(X_test_padded)


r2=r2_score(y_test, y_pred)
mse=mean_squared_error(y_test, y_pred)
# Evaluate the model
print('R2:',r2 )
print('MSE:',mse )

#plot loss graphs
plt.figure()
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()